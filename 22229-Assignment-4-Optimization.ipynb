{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "197e1cbc",
   "metadata": {},
   "source": [
    "# OPTIMIZING MODEL PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db82852",
   "metadata": {},
   "source": [
    "- Training a model is an iterative process that involves several steps.\n",
    "- In each iteration, the model makes predictions, calculates the error (loss) between its predictions and the actual targets.\n",
    "- The derivatives of the error with respect to the model's parameters are computed through backpropagation.\n",
    "- Gradient descent is then used to optimize these parameters based on the computed gradients.\n",
    "- The model repeats this process for multiple iterations, gradually improving its performance by minimizing the error and updating the parameters accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626afc16",
   "metadata": {},
   "source": [
    "## Prerequisite Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f7ae190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde18da4",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1c0b9c",
   "metadata": {},
   "source": [
    "Hyperparameters are adjustable parameters that allow us to control the model optimization process. These values can significantly influence the training and convergence rates of the model.\n",
    "\n",
    "For training, we define the following hyperparameters:\n",
    "\n",
    "1. Number of Epochs: This determines how many times the model iterates over the entire dataset during training.\n",
    "\n",
    "2. Batch Size: It indicates the number of data samples that are propagated through the network before updating the model's parameters. \n",
    "\n",
    "3. Learning Rate: This parameter controls the magnitude of updates to the model's parameters at each batch or epoch. Smaller values result in slower learning, while larger values can lead to unpredictable behavior during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57570dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc7742d",
   "metadata": {},
   "source": [
    "##  Optimization Loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbb2c5c",
   "metadata": {},
   "source": [
    "After defining the hyperparameters, we can proceed with training and optimizing our model using an optimization loop. In this process, each iteration of the optimization loop is referred to as an epoch.\n",
    "\n",
    "An epoch comprises two primary components:\n",
    "1. Train Loop: Iterate over the training dataset and optimize model parameters.\n",
    "\n",
    "2. Validation/Test Loop: Iterate over the test dataset to evaluate model performance and monitor improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a95e5a",
   "metadata": {},
   "source": [
    "## Loss Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3bd10",
   "metadata": {},
   "source": [
    "- Untrained networks often produce incorrect predictions on training data.\n",
    "- The loss function quantifies the dissimilarity between predictions and target values, which we aim to minimize during training.\n",
    "- Common loss functions are nn.MSELoss for regression tasks and nn.NLLLoss for classification tasks.\n",
    "- For multi-class classification, nn.CrossEntropyLoss combines nn.LogSoftmax and nn.NLLLoss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fc3e2e",
   "metadata": {},
   "source": [
    "**We pass our model’s output logits to nn.CrossEntropyLoss, which will normalize the logits and compute the prediction error.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f76a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6fce82",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860babc1",
   "metadata": {},
   "source": [
    "- Optimization aims to adjust model parameters to minimize model error during training.\n",
    "- Optimization algorithms dictate the specific approach for parameter adjustment.\n",
    "- The optimizer object encapsulates all the logic related to the optimization process.\n",
    "- In the example, Stochastic Gradient Descent (SGD) is used as the optimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b6ce3",
   "metadata": {},
   "source": [
    "**We initialize the optimizer by registering the model’s parameters that need to be trained, and passing in the learning rate hyperparameter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1ac7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a107164",
   "metadata": {},
   "source": [
    "Inside the training loop, optimization happens in three steps:\n",
    "\n",
    "Call optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
    "\n",
    "Backpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
    "\n",
    "Once we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088d63a1",
   "metadata": {},
   "source": [
    "## Full Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eba2cf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc1070f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.302296  [   64/60000]\n",
      "loss: 2.295867  [ 6464/60000]\n",
      "loss: 2.273243  [12864/60000]\n",
      "loss: 2.268357  [19264/60000]\n",
      "loss: 2.259188  [25664/60000]\n",
      "loss: 2.223239  [32064/60000]\n",
      "loss: 2.237178  [38464/60000]\n",
      "loss: 2.198077  [44864/60000]\n",
      "loss: 2.194753  [51264/60000]\n",
      "loss: 2.167207  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 2.165053 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.166389  [   64/60000]\n",
      "loss: 2.164572  [ 6464/60000]\n",
      "loss: 2.105277  [12864/60000]\n",
      "loss: 2.124672  [19264/60000]\n",
      "loss: 2.072789  [25664/60000]\n",
      "loss: 2.012524  [32064/60000]\n",
      "loss: 2.046674  [38464/60000]\n",
      "loss: 1.964018  [44864/60000]\n",
      "loss: 1.964232  [51264/60000]\n",
      "loss: 1.895697  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 1.898666 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.922370  [   64/60000]\n",
      "loss: 1.900261  [ 6464/60000]\n",
      "loss: 1.785633  [12864/60000]\n",
      "loss: 1.824746  [19264/60000]\n",
      "loss: 1.704462  [25664/60000]\n",
      "loss: 1.657972  [32064/60000]\n",
      "loss: 1.686977  [38464/60000]\n",
      "loss: 1.583111  [44864/60000]\n",
      "loss: 1.601208  [51264/60000]\n",
      "loss: 1.496543  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 1.521335 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.581055  [   64/60000]\n",
      "loss: 1.553708  [ 6464/60000]\n",
      "loss: 1.406751  [12864/60000]\n",
      "loss: 1.473240  [19264/60000]\n",
      "loss: 1.343467  [25664/60000]\n",
      "loss: 1.344777  [32064/60000]\n",
      "loss: 1.366823  [38464/60000]\n",
      "loss: 1.286260  [44864/60000]\n",
      "loss: 1.317399  [51264/60000]\n",
      "loss: 1.218577  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 1.248956 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.320960  [   64/60000]\n",
      "loss: 1.310221  [ 6464/60000]\n",
      "loss: 1.147111  [12864/60000]\n",
      "loss: 1.245328  [19264/60000]\n",
      "loss: 1.113666  [25664/60000]\n",
      "loss: 1.143175  [32064/60000]\n",
      "loss: 1.171729  [38464/60000]\n",
      "loss: 1.103215  [44864/60000]\n",
      "loss: 1.141542  [51264/60000]\n",
      "loss: 1.056251  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.081261 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.147808  [   64/60000]\n",
      "loss: 1.156646  [ 6464/60000]\n",
      "loss: 0.978264  [12864/60000]\n",
      "loss: 1.104338  [19264/60000]\n",
      "loss: 0.974184  [25664/60000]\n",
      "loss: 1.009138  [32064/60000]\n",
      "loss: 1.052106  [38464/60000]\n",
      "loss: 0.987927  [44864/60000]\n",
      "loss: 1.028399  [51264/60000]\n",
      "loss: 0.954325  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.974471 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.028099  [   64/60000]\n",
      "loss: 1.057931  [ 6464/60000]\n",
      "loss: 0.864114  [12864/60000]\n",
      "loss: 1.011901  [19264/60000]\n",
      "loss: 0.887267  [25664/60000]\n",
      "loss: 0.915275  [32064/60000]\n",
      "loss: 0.974127  [38464/60000]\n",
      "loss: 0.913560  [44864/60000]\n",
      "loss: 0.951099  [51264/60000]\n",
      "loss: 0.885513  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.902294 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.940315  [   64/60000]\n",
      "loss: 0.989843  [ 6464/60000]\n",
      "loss: 0.783191  [12864/60000]\n",
      "loss: 0.947702  [19264/60000]\n",
      "loss: 0.829853  [25664/60000]\n",
      "loss: 0.847208  [32064/60000]\n",
      "loss: 0.919521  [38464/60000]\n",
      "loss: 0.864582  [44864/60000]\n",
      "loss: 0.896176  [51264/60000]\n",
      "loss: 0.836257  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.850949 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.873507  [   64/60000]\n",
      "loss: 0.939765  [ 6464/60000]\n",
      "loss: 0.723444  [12864/60000]\n",
      "loss: 0.901164  [19264/60000]\n",
      "loss: 0.788984  [25664/60000]\n",
      "loss: 0.796565  [32064/60000]\n",
      "loss: 0.878625  [38464/60000]\n",
      "loss: 0.831235  [44864/60000]\n",
      "loss: 0.855294  [51264/60000]\n",
      "loss: 0.798729  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.812322 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.820601  [   64/60000]\n",
      "loss: 0.900252  [ 6464/60000]\n",
      "loss: 0.676986  [12864/60000]\n",
      "loss: 0.865892  [19264/60000]\n",
      "loss: 0.757786  [25664/60000]\n",
      "loss: 0.757759  [32064/60000]\n",
      "loss: 0.845784  [38464/60000]\n",
      "loss: 0.807129  [44864/60000]\n",
      "loss: 0.823453  [51264/60000]\n",
      "loss: 0.768689  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.781688 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab88e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
