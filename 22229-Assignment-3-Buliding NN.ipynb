{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "994fbb4d",
   "metadata": {},
   "source": [
    "# BUILD THE NEURAL NETWORK \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c92f929",
   "metadata": {},
   "source": [
    "- Neural networks consist of layers/modules that operate on data.\n",
    "- PyTorch's torch.nn namespace provides the necessary building blocks for creating custom neural networks.\n",
    "- In PyTorch, every module is a subclass of nn.Module.\n",
    "- A neural network is also a module and can contain other modules (layers) within it.\n",
    "- This nested structure allows for easy construction and management of complex network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac76df1",
   "metadata": {},
   "source": [
    "**Building a neural network to classify images in the FashionMNIST dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ac818ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b7bdc0",
   "metadata": {},
   "source": [
    "## Getting which  Device is used for  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00d43e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a32cd11",
   "metadata": {},
   "source": [
    "## Defining the Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9780fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c31b4f8",
   "metadata": {},
   "source": [
    "**creating an instance of NeuralNetwork, and moving it to the device, and printing its structure.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a42ec41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41eca1c",
   "metadata": {},
   "source": [
    "- When the model is called on the input, it produces a 2-dimensional tensor.\n",
    "- The dimension with index 0 corresponds to each output, representing 10 raw predicted values for each class.\n",
    "- The dimension with index 1 corresponds to the individual values of each output.\n",
    "- To obtain prediction probabilities, the output tensor is passed through an instance of the nn.Softmax module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6e836d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([2])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dace50",
   "metadata": {},
   "source": [
    "## Model Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5293d85",
   "metadata": {},
   "source": [
    "**breaking down the layers in the FashionMNIST model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "363824c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9cec41",
   "metadata": {},
   "source": [
    "### nn.Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba1f8b",
   "metadata": {},
   "source": [
    "To transform each 2D 28x28 image into a continuous array of 784 pixel values, we utilize the nn.Flatten layer. This layer maintains the minibatch dimension (dim=0) while reshaping the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffaeee94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d0984a",
   "metadata": {},
   "source": [
    "### nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6512d9",
   "metadata": {},
   "source": [
    "The linear layer is a module that utilizes its stored weights and biases to apply a linear transformation on the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ba0a0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e988bdda",
   "metadata": {},
   "source": [
    "### nn.ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c51d6",
   "metadata": {},
   "source": [
    "- Non-linear activations play a vital role in creating complex mappings within a neural network.\n",
    "- They are applied after linear transformations to introduce nonlinearity.\n",
    "- Nonlinear activations allow neural networks to learn a wide range of phenomena.\n",
    "- In the model mentioned, nn.ReLU activation is used between linear layers to introduce nonlinearity.\n",
    "- Other activation functions can also be used to incorporate nonlinearity in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f405cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.7782,  0.3709,  0.0203,  0.6196, -0.3334,  0.0912, -0.1739,  0.1793,\n",
      "          0.3946, -0.3149,  0.2648,  0.0742, -0.1864,  0.3381, -0.2990,  0.1212,\n",
      "         -0.0144, -0.0205, -0.0922,  0.0380],\n",
      "        [ 0.3849,  0.1957,  0.2671,  0.1751, -0.4221, -0.3151,  0.1164,  0.1764,\n",
      "         -0.0367, -0.4950,  0.3140,  0.0901, -0.6194,  0.1411, -0.0957,  0.1411,\n",
      "         -0.4975, -0.0475, -0.1031, -0.1419],\n",
      "        [ 0.3814,  0.3508,  0.2348,  0.2987, -0.3781, -0.2312,  0.0912,  0.4249,\n",
      "          0.2784, -0.0694, -0.0733, -0.1032, -0.8455,  0.4289, -0.2112,  0.5242,\n",
      "         -0.6410, -0.2007,  0.0052, -0.1337]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.7782, 0.3709, 0.0203, 0.6196, 0.0000, 0.0912, 0.0000, 0.1793, 0.3946,\n",
      "         0.0000, 0.2648, 0.0742, 0.0000, 0.3381, 0.0000, 0.1212, 0.0000, 0.0000,\n",
      "         0.0000, 0.0380],\n",
      "        [0.3849, 0.1957, 0.2671, 0.1751, 0.0000, 0.0000, 0.1164, 0.1764, 0.0000,\n",
      "         0.0000, 0.3140, 0.0901, 0.0000, 0.1411, 0.0000, 0.1411, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.3814, 0.3508, 0.2348, 0.2987, 0.0000, 0.0000, 0.0912, 0.4249, 0.2784,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.4289, 0.0000, 0.5242, 0.0000, 0.0000,\n",
      "         0.0052, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d9ce52",
   "metadata": {},
   "source": [
    "### nn.Sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ec8ff1",
   "metadata": {},
   "source": [
    "nn.Sequential is an ordered container of modules in PyTorch. It allows data to pass through each module in the same defined order, making it convenient for constructing networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4fba00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e841e3a3",
   "metadata": {},
   "source": [
    "### nn.Softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102650a2",
   "metadata": {},
   "source": [
    "The last linear layer of the neural network generates logits, which are then passed through the nn.Softmax module. This module scales the logits to obtain predicted probabilities for each class, ensuring they sum up to 1 along the specified dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ebfcd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22923da3",
   "metadata": {},
   "source": [
    "## Model Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2ead97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0059, -0.0204,  0.0241,  ..., -0.0059, -0.0171,  0.0007],\n",
      "        [-0.0141, -0.0242,  0.0274,  ..., -0.0138, -0.0138, -0.0232]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0064, -0.0191], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0034,  0.0370, -0.0198,  ..., -0.0201, -0.0402,  0.0084],\n",
      "        [-0.0140, -0.0146, -0.0221,  ...,  0.0375,  0.0093,  0.0097]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0135, -0.0144], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0434,  0.0380, -0.0377,  ...,  0.0122,  0.0291, -0.0134],\n",
      "        [ 0.0237, -0.0342, -0.0185,  ...,  0.0277, -0.0280,  0.0192]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0379,  0.0176], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4ae524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
